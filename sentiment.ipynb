{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: datasets with citations generally come from here: \n",
    "https://blog.cambridgespark.com/50-free-machine-learning-datasets-sentiment-analysis-b9388f79c124\n",
    "So, remember to cite if you use one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MORE NOTES TO SELF:\n",
    "- Find average comment length, sentiment, score, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First model: IMDB  \n",
    "From:  \n",
    "Maas, A., Daly, R., Pham, P., Huang, D., Ng, A. and Potts, C. (2011). Learning Word Vectors for Sentiment Analysis: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. [online] Portland, Oregon, USA: Association for Computational Linguistics, pp.142â€“150. Available at: http://www.aclweb.org/anthology/P11-1015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB reviews may be applicable considering they are 1) comparable to the length of a reddit comment and 2) written about movies, obviously comparable to writing about TV shows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset comes in the form of a neg folder and a pos folder, each with comments in .txt format from IMDB. So, I'll need to put them in a more useful format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def folders_to_df(pos_path_as_str, neg_path_as_str):\n",
    "    \n",
    "    lst = []\n",
    "    \n",
    "    pos_path = os.fsencode(pos_path_as_str)\n",
    "    neg_path = os.fsencode(neg_path_as_str)\n",
    "    \n",
    "    for file in os.listdir(pos_path):\n",
    "        \n",
    "        filepath = pos_path_as_str + \"\\\\\" + os.fsdecode(file)\n",
    "        with open(filepath, \"r\", encoding=\"utf8\") as f:\n",
    "            body = f.read()\n",
    "            lst.append([body, 1])\n",
    "            \n",
    "    for file in os.listdir(neg_path):\n",
    "        \n",
    "        filepath = neg_path_as_str + \"\\\\\" + os.fsdecode(file)\n",
    "        with open(filepath, \"r\", encoding=\"utf8\") as f:\n",
    "            body = f.read()\n",
    "            lst.append([body, -1])\n",
    "            \n",
    "    return pd.DataFrame(lst, columns = [\"body\", \"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_path = r\"C:\\Users\\Spencer\\Desktop\\sentiment_datasets\\aclImdb\\train\\pos\"\n",
    "train_neg_path = r\"C:\\Users\\Spencer\\Desktop\\sentiment_datasets\\aclImdb\\train\\neg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = folders_to_df(train_pos_path, train_neg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  sentiment\n",
       "0  Bromwell High is a cartoon comedy. It ran at t...          1\n",
       "1  Homelessness (or Houselessness as George Carli...          1\n",
       "2  Brilliant over-acting by Lesley Ann Warren. Be...          1\n",
       "3  This is easily the most underrated film inn th...          1\n",
       "4  This is not the typical Mel Brooks film. It wa...          1"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is already split into train/test, but I'm combining them so I can define their ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos_path = r\"C:\\Users\\Spencer\\Desktop\\sentiment_datasets\\aclImdb\\test\\pos\"\n",
    "test_neg_path = r\"C:\\Users\\Spencer\\Desktop\\sentiment_datasets\\aclImdb\\test\\neg\"\n",
    "\n",
    "testing_set = folders_to_df(test_pos_path, test_neg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  sentiment\n",
       "0  I went and saw this movie last night after bei...          1\n",
       "1  Actor turned director Bill Paxton follows up h...          1\n",
       "2  As a recreational golfer with some knowledge o...          1\n",
       "3  I saw this film in a sneak preview, and it is ...          1\n",
       "4  Bill Paxton has taken the true story of the 19...          1"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset = training_set.append(testing_set, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      "body         50000 non-null object\n",
      "sentiment    50000 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 781.3+ KB\n"
     ]
    }
   ],
   "source": [
    "imdb_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def text_processor(text):\n",
    "    punc_removed = ''.join([char for char in text if char not in string.punctuation])\n",
    "    return [word.lower() for word in punc_removed.split() if word.lower() not in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Spencer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Spencer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Spencer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def full_processor(text_string):\n",
    "    \n",
    "    # tokenize text\n",
    "    tokenized_text = word_tokenize(text_string)\n",
    "    \n",
    "    # remove stopwords and punctuation\n",
    "    no_stops = []\n",
    "    for word in tokenized_text:\n",
    "        if (word not in stop) and (word not in string.punctuation) :\n",
    "            no_stops.append(word)\n",
    "    \n",
    "    # get parts of speech and lemmatize\n",
    "    pos_ = {\"N\": wordnet.NOUN,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"J\": wordnet.ADJ,\n",
    "            \"R\": wordnet.ADV}\n",
    "    lemmatized = []\n",
    "    tags = nltk.pos_tag(tokenized_text)\n",
    "    for word in tags:\n",
    "        tag = word[1][0]\n",
    "        if tag not in pos_: # anything not in .lemmatize()'s extremely limited domain is considered a noun\n",
    "            tag = 'N'\n",
    "        tag = pos_[tag]\n",
    "        lem = lemmatizer.lemmatize(word[0], tag)\n",
    "        lemmatized.append(lem)\n",
    "    \n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_cv = CountVectorizer(analyzer=full_processor).fit_transform(imdb_dataset[\"body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imdb_cv_nb, X_test_imdb_cv_nb, y_train_imdb_cv_nb, y_test_imdb_cv_nb = train_test_split(imdb_cv, imdb_dataset['sentiment'], test_size = 0.25, random_state = 137)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_cv_nb = MultinomialNB().fit(X_train_imdb_cv_nb, y_train_imdb_cv_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_pred_cv_nb = imdb_cv_nb.predict(X_test_imdb_cv_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.87      0.85      6345\n",
      "           1       0.86      0.82      0.84      6155\n",
      "\n",
      "   micro avg       0.84      0.84      0.84     12500\n",
      "   macro avg       0.84      0.84      0.84     12500\n",
      "weighted avg       0.84      0.84      0.84     12500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_imdb_cv_nb, imdb_pred_cv_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_tfidf = TfidfVectorizer(analyzer = full_processor).fit_transform(imdb_dataset[\"body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imdb_tfidf_nb, X_test_imdb_tfidf_nb, y_train_imdb_tfidf_nb, y_test_imdb_tfidf_nb = train_test_split(imdb_tfidf, imdb_dataset['sentiment'], test_size = 0.3, random_state = 57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_tfidf_nb = MultinomialNB().fit(X_train_imdb_tfidf_nb, y_train_imdb_tfidf_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_pred_tfidf_nb = imdb_tfidf_nb.predict(X_test_imdb_tfidf_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.85      0.88      0.87      7601\n",
      "           1       0.87      0.84      0.86      7399\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     15000\n",
      "   macro avg       0.86      0.86      0.86     15000\n",
      "weighted avg       0.86      0.86      0.86     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_imdb_tfidf_nb, imdb_pred_tfidf_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.91      0.90      0.91      6309\n",
      "           1       0.90      0.91      0.91      6191\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     12500\n",
      "   macro avg       0.91      0.91      0.91     12500\n",
      "weighted avg       0.91      0.91      0.91     12500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_imdb_tfidf_svc, X_test_imdb_tfidf_svc, y_train_imdb_tfidf_svc, y_test_imdb_tfidf_svc = train_test_split(imdb_tfidf, imdb_dataset['sentiment'], test_size = 0.25, random_state = 49)\n",
    "imdb_tfidf_svc = LinearSVC().fit(X_train_imdb_tfidf_svc, y_train_imdb_tfidf_svc)\n",
    "imdb_pred_tfidf_svc = imdb_tfidf_svc.predict(X_test_imdb_tfidf_svc)\n",
    "print(classification_report(y_test_imdb_tfidf_svc, imdb_pred_tfidf_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strangely high accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Spencer\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.py:102: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "C:\\Users\\Spencer\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.py:102: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "imdb_hv = HashingVectorizer(analyzer = full_processor, non_negative = True).fit_transform(imdb_dataset[\"body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.90      0.89      0.90      7551\n",
      "           1       0.89      0.90      0.90      7449\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     15000\n",
      "   macro avg       0.90      0.90      0.90     15000\n",
      "weighted avg       0.90      0.90      0.90     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_imdb_hv_svc, X_test_imdb_hv_svc, y_train_imdb_hv_svc, y_test_imdb_hv_svc = train_test_split(imdb_hv, imdb_dataset['sentiment'], test_size = 0.3, random_state = 799)\n",
    "imdb_hv_svc = LinearSVC().fit(X_train_imdb_hv_svc, y_train_imdb_hv_svc)\n",
    "imdb_pred_hv_svc = imdb_hv_svc.predict(X_test_imdb_hv_svc)\n",
    "print(classification_report(y_test_imdb_hv_svc, imdb_pred_hv_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.80      0.91      0.85      7543\n",
      "           1       0.89      0.77      0.83      7457\n",
      "\n",
      "   micro avg       0.84      0.84      0.84     15000\n",
      "   macro avg       0.85      0.84      0.84     15000\n",
      "weighted avg       0.85      0.84      0.84     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_imdb_hv_nb, X_test_imdb_hv_nb, y_train_imdb_hv_nb, y_test_imdb_hv_nb = train_test_split(imdb_hv, imdb_dataset['sentiment'], test_size = 0.3, random_state = 709)\n",
    "imdb_hv_nb = MultinomialNB().fit(X_train_imdb_hv_nb, y_train_imdb_hv_nb)\n",
    "imdb_pred_hv_nb = imdb_hv_nb.predict(X_test_imdb_hv_nb)\n",
    "print(classification_report(y_test_imdb_hv_nb, imdb_pred_hv_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [imdb_cv_nb, imdb_tfidf_nb, imdb_tfidf_svc, imdb_hv_svc, imdb_hv_nb]\n",
    "strings = [\"imdb_cv_nb\", \"imdb_tfidf_nb\", \"imdb_tfidf_svc\", \"imdb_hv_svc\", \"imdb_hv_nb\"]\n",
    "for i in range(len(models)):\n",
    "    pickle.dump(models[i], open(strings[i] + \".sav\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll try Sentiment140  \n",
    "\n",
    "Go, A., Bhayani, R. and Huang, L., 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, 1(12)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "s140test = pd.read_csv(r\"C:\\Users\\Spencer\\Desktop\\sentiment_datasets\\trainingandtestdata\\testdata.manual.2009.06.14.csv\", encoding = \"ISO-8859-1\", names = [\"polarity\", \"id\", \"date\", \"topic\", \"username\", \"body\"])\n",
    "s140train = pd.read_csv(r\"C:\\Users\\Spencer\\Desktop\\sentiment_datasets\\trainingandtestdata\\training.1600000.processed.noemoticon.csv\", encoding = \"ISO-8859-1\", names = [\"polarity\", \"id\", \"date\", \"topic\", \"username\", \"body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    182\n",
       "0    177\n",
       "2    139\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s140test[\"polarity\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's my understanding from these numbers and their distribution (and the fact that the original study is using a positive, negative, neutral system) that 0 is negative, 2 is neutral, 4 is positive. The tweets themselves seem to indicate that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@stellargirl I loooooooovvvvvveee my Kindle2. Not that the DX is cool, but the 2 is fantastic in its own right.'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 looks positive\n",
    "s140test[s140test[\"polarity\"]==4][\"body\"].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Check this video out -- President Obama at the White House Correspondents' Dinner http://bit.ly/IMXUM\""
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 looks neutral\n",
    "s140test[s140test[\"polarity\"]==2][\"body\"].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fuck this economy. I hate aig and their non loan given asses.'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 looks negative\n",
    "s140test[s140test[\"polarity\"]==0][\"body\"].tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, I'll reassign them using the system I'm using for the sake of consistency. I'll also combine test and train so I can split them differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "s140 = s140test.append(s140train, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "s140[\"polarity\"] = s140[\"polarity\"].apply(lambda x: int((x / 2) - 1)) # 4->1, 2->0, 0->-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    800182\n",
       "-1    800177\n",
       " 0       139\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s140[\"polarity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a long time\n",
    "s140_cv = CountVectorizer(analyzer=full_processor).fit_transform(s140[\"body\"])\n",
    "s140_tfidf = TfidfVectorizer(analyzer=full_processor).fit_transform(s140[\"body\"])\n",
    "s140_hv = HashingVectorizer(analyzer=full_processor).fit_transform(s140[\"body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(s140_cv, s140[\"polarity\"], test_size = 0.4, random_state = 135)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restarting computer soon so I'll save previously made models so they won't need to be retrained due to notebook restraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_cv_filename = \"imdb_cv_model.sav\"\n",
    "imdb_tfidf_filename = \"imdb_tfidf_model.sav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imdb_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-6bcd646504ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimdb_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimdb_cv_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimdb_model_v2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimdb_tfidf_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'imdb_model' is not defined"
     ]
    }
   ],
   "source": [
    "pickle.dump(imdb_model, open(imdb_cv_filename, \"wb\"))\n",
    "pickle.dump(imdb_model_v2, open(imdb_tfidf_filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "s140_cv_model = MultinomialNB().fit(X_train_3, y_train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_3 = s140_cv_model.predict(X_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.76      0.81      0.78    319714\n",
      "           0       0.00      0.00      0.00        54\n",
      "           1       0.79      0.74      0.76    320432\n",
      "\n",
      "   micro avg       0.77      0.77      0.77    640200\n",
      "   macro avg       0.52      0.51      0.51    640200\n",
      "weighted avg       0.77      0.77      0.77    640200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_3, test_predictions_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "s140_cv_filename = \"s140_cv_model.sav\"\n",
    "pickle.dump(s140_cv_model, open(s140_cv_filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "s140_tfidf = TfidfVectorizer(analyzer = full_processor).fit_transform(s140[\"body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_4, X_test_4, y_train_4, y_test_4 = train_test_split(s140_cv, s140[\"polarity\"], test_size = 0.3, random_state = 93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "s140_tfidf_model = MultinomialNB().fit(X_train_4, y_train_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_4 = s140_tfidf_model.predict(X_test_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.76      0.81      0.78    239708\n",
      "           0       0.00      0.00      0.00        38\n",
      "           1       0.79      0.74      0.77    240404\n",
      "\n",
      "   micro avg       0.77      0.77      0.77    480150\n",
      "   macro avg       0.52      0.52      0.52    480150\n",
      "weighted avg       0.77      0.77      0.77    480150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_4, test_predictions_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I should preprocess all the text so its lemmatized and everything before fitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
